{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training a Neural Network using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path, pandas as pd \n",
    "import matplotlib as mp, matplotlib.pyplot as plt\n",
    "import numpy as np, json, random, ast\n",
    "from IPython.display import clear_output\n",
    "from pandas.io.json import json_normalize, read_json\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import cesiumpy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "import sys\n",
    "\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Set some parameters for nicer visualizations\n",
    "pd.set_option('display.expand_frame_repr', False) #do not wrap the printout of Pandas DataFrames\n",
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "mp.rcParams['figure.figsize'] = (15, 15)\n",
    "mp.pyplot.style.use = 'fivethirtyeight'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7110, 11) (7110, 1)\n"
     ]
    }
   ],
   "source": [
    "#get data, normalize it, check dimensions\n",
    "dfmain = pd.read_csv('data/dffinal.csv', index_col=0)\n",
    "dfmain['geojson'] = dfmain['geojson'].map(ast.literal_eval) #convert string to dict\n",
    "dfmain['sta'] = pd.to_datetime(dfmain['sta'])\n",
    "dfmain['end'] = pd.to_datetime(dfmain['end'])\n",
    "dfmain['duration'] = pd.to_timedelta(dfmain['duration'])   \n",
    "dfmain.sample()\n",
    "\n",
    "X = np.array(dfmain[['humidity', 'dewPoint', 'temperature', \n",
    "                    'pressure', 'visibility', 'windSpeed', \n",
    "                    'precipIntensity', 'precipProbability', \n",
    "                    'dat.cape', 'crosswind', 'ratio']])\n",
    "y = np.array(dfmain['regulated'], dtype='float')\n",
    "\n",
    "y.astype('float')\n",
    "\n",
    "X_train, X_test, y_train, y_test = skl.model_selection.train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# Apply SMOTE on the training set:\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "smo = SMOTE(ratio='all', kind='regular', random_state=42)\n",
    "X_train, y_train = smo.fit_sample(X_train, y_train)\n",
    "\n",
    "adas = ADASYN(ratio='minority', random_state=42)\n",
    "X_train, y_train = adas.fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "y.shape = (y.size, 1)\n",
    "y_train.shape = (y_train.size, 1)\n",
    "y_test.shape = (y_test.size, 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate y vectors from [0] to [0 1]\n",
    "y_train = np.concatenate([1 - y_train, y_train], axis =1 )\n",
    "y_test = np.concatenate([1 - y_test, y_test], axis =1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "adapted from\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "training_epochs = 110\n",
    "batch_size = 400\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 15 # 1st layer number of features\n",
    "n_hidden_2 = 15 # 2nd layer number of features\n",
    "n_hidden_3 = 15\n",
    "n_input = 11 # number of features\n",
    "n_classes = 2 # number of class variables (1 means one variable with possible values 0 and 1)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# model factory\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.tanh(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_3 = tf.add(tf.matmul(layer_1, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss, optimizer and AUC\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "_, aucscore = tf.metrics.auc(labels=tf.argmax(y, 1), predictions=tf.argmax(pred, 1) )\n",
    "\n",
    "# UPDATE: modify cost function such that the imbalanced dataset is respected\n",
    "class_weights = tf.constant([[1.0, 12.0]])\n",
    "costweights =  tf.reduce_sum(class_weights * y, axis = 1)\n",
    "weighted_cost = cost * costweights\n",
    "cost = weighted_cost    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 aucscore= 0.500000000\n",
      "Epoch: 0011 aucscore= 0.734552205\n",
      "Epoch: 0021 aucscore= 0.802620471\n",
      "Epoch: 0031 aucscore= 0.833911777\n",
      "Epoch: 0041 aucscore= 0.853651404\n",
      "Epoch: 0051 aucscore= 0.867936492\n",
      "Epoch: 0061 aucscore= 0.879143000\n",
      "Epoch: 0071 aucscore= 0.887951374\n",
      "Epoch: 0081 aucscore= 0.895048678\n",
      "Epoch: 0091 aucscore= 0.900943100\n",
      "Epoch: 0101 aucscore= 0.906152189\n",
      "Optimization Finished!\n",
      "Accuracy: 0.880159\n",
      "0.462785\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "#init2 = tf.local_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    sess.run(tf.initialize_local_variables())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(X_train.size / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            start = i * batch_size\n",
    "            end =  i * batch_size + batch_size\n",
    "            X_train_batch = X_train[start:end, :]\n",
    "            y_train_batch = y_train[start:end, :]            \n",
    "            \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            result = sess.run([optimizer, cost], feed_dict={x: X_train_batch, y: y_train_batch})\n",
    "            # Compute average aucscore on the batch\n",
    "            # sess.run(tf.initialize_local_variables())\n",
    "            aucs = sess.run(aucscore, feed_dict={x : X_train_batch, y : y_train_batch})\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"aucscore=\", \"{:.9f}\".format(aucs))\n",
    "            sys.stdout.flush()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Evaluation:\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: X_test, y: y_test}))\n",
    "    \n",
    "    # print final AUC\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    sess.run(tf.initialize_local_variables())\n",
    "    print(sess.run(aucscore, feed_dict={x : X_test, y : y_test} ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacron1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
