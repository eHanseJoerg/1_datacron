{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from the data\n",
    "\n",
    "In the last notebook (5.3), we created the feature vector that we want to use to learn the conditions that lead to regulations. In this notebook, we will load the dataset, select the most promising features, normalize and scale them, and finally run a classifier on them.\n",
    "\n",
    "As the goal of the thesis is to discuss the learnability of the problem and not to acheive production-grade performance, we will use the well-known basic algorithms provided by the Python package scikit-learn (www.scikit-learn.org). The following picture from scikit-learn.org provides a \"path\" through the maze of algorithms and hints us that we should try Linear SVC, KNeighboursClassifier or SVC:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ml_map.png\" size=40%></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Some preconsiderations\n",
    "\n",
    "In the last notebok (5.3) we made some basic analysis on the available wx and other parameters. Deriving from these insights, we could conclude that we are 1) dealing with an imbalanced problem, and 2) some wx parameters did not play a role for our subset of data from April 2016 (e.g. storm data). \n",
    "\n",
    "## 1) Dealing with an imbalanced problem\n",
    "\n",
    "If, like in our case, the problem is imbalanced so that 94% of the entries belong to one class, a completely naive classifier that would just classify all items as belonging to the majority class, would acheive an accuracy of 94%, but would classify 100% of the minority entries wrong. In many applications (e.g. fraud detection), this minority class contains the more important events. The same applies to our problem, as has been commented by CRIDA in our interview. Therefore, the estimator should have the tendency to capture as many of the critical entries as possible, even at the cost of misclassifying uncritical entries. The following definitions help to formalize the problem and select the correct scoring methods:\n",
    "\n",
    " $$ \\ prec \\  and  \\ recall $$\n",
    "\n",
    "\n",
    "Apart from the correct analysis, different algorithmic methods are available to deal with imbalanced problems. Some examples are oversampling and undersampling. We are going to use a vanilla and an oversampled dataset to compare the results of the oversampling method. Oversampling works as follows:\n",
    "\n",
    "  $$ \\ demonstrate \\ oversampling $$\n",
    "\n",
    "\n",
    "\n",
    "## 2) Dealing with non-relevant features\n",
    "\n",
    "Our dataset contains only April 2016. As the Iberian peninsula is subject to seasonal climate changes, we do not have all possible weather conditions in our sample dataset. For example, we do not have storm conditions or freezing precipitation in our dataset. An experiment on a larger data basis should consider these variables, for our purpos here, we will omit them.\n",
    "\n",
    "## 3) Other considerations\n",
    "\n",
    "The scikit-learn API makes it very easy to apply advanced learning methods to our model. We can perform a Grid Search on the hyperparameters so that the best hyperparameters are found for us. It is easy to implement cross-validation into the learning process. We plan to use these features provided by the API. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import ast\n",
    "from pandas.io.json import json_normalize, read_json\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Set some parameters for nicer visualizations\n",
    "pd.set_option('display.expand_frame_repr', False) #do not wrap the printout of Pandas DataFrames\n",
    "pd.set_option('display.precision', 2)\n",
    "mp.rcParams['figure.figsize'] = (15, 12)\n",
    "mp.pyplot.style.use = 'fivethirtyeight'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icao</th>\n",
       "      <th>s</th>\n",
       "      <th>iata</th>\n",
       "      <th>wkt</th>\n",
       "      <th>geojson</th>\n",
       "      <th>sta</th>\n",
       "      <th>end</th>\n",
       "      <th>duration</th>\n",
       "      <th>rwys</th>\n",
       "      <th>wx</th>\n",
       "      <th>...</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>windGust</th>\n",
       "      <th>precipIntensity</th>\n",
       "      <th>precipProbability</th>\n",
       "      <th>storm</th>\n",
       "      <th>crosswind</th>\n",
       "      <th>cap</th>\n",
       "      <th>demand</th>\n",
       "      <th>ratio</th>\n",
       "      <th>regulated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEAL</td>\n",
       "      <td>Place_Alicante_Elche_Airport</td>\n",
       "      <td>ALC</td>\n",
       "      <td>POINT (-0.558055579662323 38.282222747802734)</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-0.558055579...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-01 00:59:59</td>\n",
       "      <td>00:59:59</td>\n",
       "      <td>280</td>\n",
       "      <td>{'latitude': 38.28, 'longitude': -0.56, 'timez...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.02</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LEBL</td>\n",
       "      <td>Place_Barcelona_ElPrat_Airport</td>\n",
       "      <td>BCN</td>\n",
       "      <td>POINT (2.0783333778381348 41.29694366455078)</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [2.0783333778...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-01 00:59:59</td>\n",
       "      <td>00:59:59</td>\n",
       "      <td>250</td>\n",
       "      <td>{'latitude': 41.3, 'longitude': 2.08, 'timezon...</td>\n",
       "      <td>...</td>\n",
       "      <td>4.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEBB</td>\n",
       "      <td>Place_Bilbao___Sondica</td>\n",
       "      <td>BIO</td>\n",
       "      <td>POINT (-2.910555601119995 43.301109313964844)</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-2.910555601...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-01 00:59:59</td>\n",
       "      <td>00:59:59</td>\n",
       "      <td>300</td>\n",
       "      <td>{'latitude': 43.3, 'longitude': -2.91, 'timezo...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.79</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEMD</td>\n",
       "      <td>Place_Madrid_Barajas_Airport</td>\n",
       "      <td>MAD</td>\n",
       "      <td>POINT (-3.560833215713501 40.47222137451172)</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-3.560833215...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-01 00:59:59</td>\n",
       "      <td>00:59:59</td>\n",
       "      <td>180</td>\n",
       "      <td>{'latitude': 40.47, 'longitude': -3.56, 'timez...</td>\n",
       "      <td>...</td>\n",
       "      <td>5.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LEMG</td>\n",
       "      <td>Place_Malaga_CostaDelSol_Airport</td>\n",
       "      <td>AGP</td>\n",
       "      <td>POINT (-4.499166488647461 36.67499923706055)</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-4.499166488...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-04-01 00:59:59</td>\n",
       "      <td>00:59:59</td>\n",
       "      <td>320</td>\n",
       "      <td>{'latitude': 36.67, 'longitude': -4.5, 'timezo...</td>\n",
       "      <td>...</td>\n",
       "      <td>6.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.27</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   icao                                 s iata                                            wkt                                            geojson        sta                 end duration  rwys                                                 wx    ...      windSpeed  windGust  precipIntensity  precipProbability  storm  crosswind  cap  demand  ratio  regulated\n",
       "0  LEAL      Place_Alicante_Elche_Airport  ALC  POINT (-0.558055579662323 38.282222747802734)  {'type': 'Point', 'coordinates': [-0.558055579... 2016-04-01 2016-04-01 00:59:59 00:59:59   280  {'latitude': 38.28, 'longitude': -0.56, 'timez...    ...           4.03       0.0              0.0                0.0      0       2.02   34     0.0    0.0          0\n",
       "1  LEBL    Place_Barcelona_ElPrat_Airport  BCN   POINT (2.0783333778381348 41.29694366455078)  {'type': 'Point', 'coordinates': [2.0783333778... 2016-04-01 2016-04-01 00:59:59 00:59:59   250  {'latitude': 41.3, 'longitude': 2.08, 'timezon...    ...           4.28       0.0              0.0                0.0      0       2.20   48     0.0    0.0          0\n",
       "2  LEBB            Place_Bilbao___Sondica  BIO  POINT (-2.910555601119995 43.301109313964844)  {'type': 'Point', 'coordinates': [-2.910555601... 2016-04-01 2016-04-01 00:59:59 00:59:59   300  {'latitude': 43.3, 'longitude': -2.91, 'timezo...    ...           3.58       0.0              0.0                0.0      0       1.79   22     0.0    0.0          0\n",
       "3  LEMD      Place_Madrid_Barajas_Airport  MAD   POINT (-3.560833215713501 40.47222137451172)  {'type': 'Point', 'coordinates': [-3.560833215... 2016-04-01 2016-04-01 00:59:59 00:59:59   180  {'latitude': 40.47, 'longitude': -3.56, 'timez...    ...           5.04       0.0              0.0                0.0      0       0.79   38     0.0    0.0          0\n",
       "4  LEMG  Place_Malaga_CostaDelSol_Airport  AGP   POINT (-4.499166488647461 36.67499923706055)  {'type': 'Point', 'coordinates': [-4.499166488... 2016-04-01 2016-04-01 00:59:59 00:59:59   320  {'latitude': 36.67, 'longitude': -4.5, 'timezo...    ...           6.55       0.0              0.0                0.0      0       3.27   37     0.0    0.0          0\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfmain = pd.read_csv('data/dffinal.csv', index_col=0)\n",
    "dfmain['geojson'] = dfmain['geojson'].map(ast.literal_eval) #convert string to dict\n",
    "dfmain['sta'] = pd.to_datetime(dfmain['sta'])\n",
    "dfmain['end'] = pd.to_datetime(dfmain['end'])\n",
    "dfmain['duration'] = pd.to_timedelta(dfmain['duration'])   \n",
    "dfmain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    " - implementiere die Pipeline als Standardweg: Normalisierung, PCA, SVC.\n",
    " - du hast ein unbalanced problem. Dem musst du Rechnung tragen, und bei score() entsprechend nicht die accuracy nehmen, sondern precision and recall!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization has the purpose of leveling different features into a comparable numerical dimension. While _scaling_ only levels the numerical range of the features, _normalization_ refers to a transformation that generates an input matrix where the features have 0 mean and equal variance across the observations. We will be using `sklearn` for this purpose. In `sklearn`, two different classes implement scaling and normalization, respectively: `MinMaxScaler` and `StandardScaler`. The scaling class operates as follows: $$ \\frac{1}{2} $$\n",
    "\n",
    "The normalization class operates as follows:\n",
    "\n",
    "As described in the `sklearn` documentation, scaling is more common to preprocess data for Neural Nets, while normalization is being used for Support Vector Machines and alike.\n",
    "\n",
    "In the follwoing code sections, $X$ will always denote features, and $y$ will denote the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "X, y = np.array(df_fl[['windSpeed', 'crosswind', 'ratio']]),  np.array(df_fl['regulated'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) \n",
    "\n",
    "#note to self: instead of train_test_split, we can use kfold for cross-validation\n",
    "\n",
    "\n",
    "#instantiate a normalizer. Beware to fit on train, and only tranform on test.\n",
    "normer = preprocessing.Normalizer()\n",
    "X_train_norm = normer.fit_transform(X_train)\n",
    "X_test_norm  = normer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70, 3), (30, 3), (70,), (30,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.shape, X_test_norm.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fitting a Support Vector Classifier\n",
    "\n",
    "We will now use the prepared dataset to fit a SVC to the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98        29\n",
      "          1       0.00      0.00      0.00         1\n",
      "\n",
      "avg / total       0.93      0.97      0.95        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joerg/anaconda3/envs/datacron1/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a logistic regression model\n",
    "\n",
    "clf_base = LogisticRegression()\n",
    "grid = {'C': 10.0 ** np.arange(-2,3), penalty: {'11','12'}}\n",
    "\n",
    "cv = KFold(X_train.shape[0], n_folds=5, shuffle=True, random_state=0)\n",
    "clf = GridSearchCV(clf_base, grid, cv=cv, n_jobs=8, scoring='f1_macro')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "coef = clf.best_estimator_.coef_\n",
    "intercept = clf.best_estimator_.intercept_\n",
    "\n",
    "\n",
    "\n",
    "#Ausführlicher Testreport:\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(y_test, clf.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "SMOTE oversampling technique:\n",
    "\n",
    " - paper lesen\n",
    " - ist eine datenorientierte resampling technik, um mit imbalance zu copen\n",
    " \n",
    " os = SMOTE(ratio=0.5, k=5, random_state=1)\n",
    " X_train_res, y_train_res = os.fit_sample(X_train, y_train)\n",
    " \n",
    " \n",
    ".... danach hat man ein größeres Dataset und kann die ML Classifier ganz normal anwenden\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline from sklearn.decomposition import PCA, KernelPCA \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "union = FeatureUnion([(\"pca\", PCA()), (\"kpca\", KernelPCA(kernel=\"rbf\"))])\n",
    "Pipeline([(\"feat_union\", union), \n",
    "          (\"feat_sel\", SelectKBest(k=10)), \n",
    "          (\"log_reg\", LogisticRegression(penalty=\"l2\")) ]).fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statt GridSearchCV könnte ich auch RandomizedSearchCV nehmen. Diese Algorithmen sind dann auch für die Cross-Validation (k-fold) und die score functions zuständig. The score function used by default is the estimator’s score method, but the library provides a variety of alternatives that the user can choose from, including accuracy, AUC and F1 score for classification, R2 score and mean squared error for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.svm import SVC\n",
    "param_grid = [ {\"kernel\": [\"linear\"], \"C\": [1, 10, 100, 1000]}, \n",
    "               {\"kernel\": [\"rbf\"], \"C\": [1, 10, 100, 1000], \"gamma\": [0.001, 0.0001]} ]\n",
    "clf = GridSearchCV(SVC(), param_grid, scoring=\"f1\", cv=10) \n",
    "clf.fit(X_train, y_train) y_pred = clf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacron1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
